В интернете одни сайтыы часто ссылаются на другие сайты. Это одна из фундаментальных концепций в интернете называемая гипертекст. Однако существует проблема, ссылаяся на другие сайты мы не можем гарантировать их работоспособности. В процессе жизни может произойти такое, что страница окажется недоступна, либо сайт начнет постоянно выдавать ошибку. Это довольно серьезная проблема и существует даже специальные сервисы, которые помогают находить "битые" страницы.

Одним из таких сервисов пользуется и сам Хекслет.

# checker.js
Реализуйте и экспортируйте по умолчанию функцию, которая принимает на вход ссылку на страницу какого-то сайта, загружает содержимое этой страницы, извлекает из него ссылки и проверяет их доступность. Функция должна вернуть список битых ссылок.

```js
import getBadLinks from '../checker.js';

const url = 'https://privet.hexlet';
const links = await getBadLinks(url);
console.log(links);

// Гипотетический пример:
// [
//      'https://privet.hexlet/somepage',
//      'https://privet.hexlet/another/page',
// ]
```
Проверка доступности:
Любые ссылки возвращающие коды ответа кроме 2хх (любые двухсотые) считаются битыми. Axios по умолчанию считает ошибочными все коды кроме 2хх и отправляет их в блок catch(). То есть если запрос axios.get() отработал без ошибок, то страница отдает 2хх код ответа, если завершился с ошибкой, то ответа либо нет, либо это не 2хх код. Такие ссылки считаются битыми.

## Подсказки
* Для извлечения ссылок со страницы воспользуйтесь функцией extractLinks(content)